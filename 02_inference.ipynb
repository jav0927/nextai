{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "  !pip install git+https://github.com/pete88b/nbdev_colab_helper.git --quiet\n",
    "  from nbdev_colab_helper.core import *\n",
    "  project_name = 'nextai'\n",
    "  init_notebook(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # inference\n",
    "\n",
    "> Predict outcomes in object recognition projects.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r\u001b[K     |█                               | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 1.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 174kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 204kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 215kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 245kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 276kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 286kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 307kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 317kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 327kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 348kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 3.0MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "!pip install fastai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/github/nextai/nextai_lib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/github/nextai/nextai_lib'"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "%cd './nextai_lib/'\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "#from fastai.vision.all import *\n",
    "from fastai import *\n",
    "from typing import *\n",
    "from torch import tensor, Tensor\n",
    "import torch\n",
    "import torchvision      # Needed to invoke torchvision.ops.mns function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Automatically sets for GPU or CPU environments\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference calculations are divided into three main modules:\n",
    "\n",
    "*   get_activ_offsets_mns - Calculate offsets of activation boxes with respect to anchors and pass activation boxes through MNS\n",
    "*   pad_output - Normalize the output tensors to allow them to be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Pad tensors so that they have uniform dimentions: (batch size, no of items in a batch, 4) and  (batch size, no of items in a batch, 21)\n",
    "def pad_output(l_bb:List, l_scr:List, l_idx:List, no_classes:int):\n",
    "  '''Pad tensors so that they have uniform dimentions: (batch size, no of items in a batch, 4) and  (batch size, no of items in a batch, 21)\n",
    "      Inputs: l_bb  - list  of tensors containing individual non-uniform sized bounding boxes\n",
    "              l_scr - list  of tensors containing class index values (i.e. 1 - airplane)\n",
    "              l_idx - list of tensors containing class index values (i.e. 1 - airplane)\n",
    "              no_classes - Number of classes, Integer\n",
    "      Outputs: Uniform-sized tensors: bounding box tensor and score tensor with dims: (batch size, no of items in a batch, 4) and  (batch size, no of items in a batch, 21)'''\n",
    "\n",
    "  if len([len(img_bb) for img_bb in l_bb]) == 0.: \n",
    "    print(F'Image did not pass the scoring threshold')\n",
    "    return\n",
    "\n",
    "  mx_len = max([len(img_bb) for img_bb in l_bb])                  # Calculate maximun lenght of the boxes in the batch\n",
    "\n",
    "  l_b, l_c, l_x, l_cat = [], [], [], []\n",
    "  # Create Bounding Box tensors                                      # zeroed tensor accumulators  \n",
    "  for i, ntr in enumerate(zip(l_bb, l_scr, l_idx)):    \n",
    "    bbox, cls, idx = ntr[0], ntr[1], ntr[2]                     # Unpack variables\n",
    "    tsr_len = mx_len - bbox.shape[0]                            # Calculate the number of zero-based rows to add                \n",
    "    m = nn.ConstantPad2d((0, 0, 0, tsr_len), 0.)                # Prepare to pad the box tensor with zero entries                                          \n",
    "    l_b.append(m(bbox))                                         # Add appropriate zero-based box rows and add to list\n",
    "\n",
    "    # Create Category tensors\n",
    "    cat_base = torch.zeros(mx_len-bbox.shape[0], dtype=torch.int32)  \n",
    "    img_cat = torch.cat((idx, cat_base), dim=0) \n",
    "    l_cat.append(img_cat) \n",
    "\n",
    "    # Create Score tensors\n",
    "    img_cls = []                                                # List to construct class vectors\n",
    "    for ix in range(idx.shape[0]):                              # Construct class vectors of dim(no of classes)\n",
    "        cls_base = torch.zeros(no_classes).to(device)           # Base zero-based class vector                    \n",
    "        cls_base[idx[ix]] = cls[ix]                             # Add the score in the nth position\n",
    "        img_cls.append(cls_base)\n",
    "    img_stack = torch.stack(img_cls)                            # Create single tensor per image\n",
    "    img_stack_out =  m(img_stack) \n",
    "    l_c.append( img_stack_out )                                 # Add appropriate zero-based class rows and add to list\n",
    "\n",
    "  return (TensorBBox(torch.stack(l_b,0)), TensorMultiCategory(torch.stack(l_c,0)), TensorMultiCategory(torch.stack(l_cat,0)) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def get_activ_offsets_mns(anchrs:Tensor, activs:Tensor, no_classes:int, threshold:float=0.5):\n",
    "  ''' Takes in activations and calculates corresponding anchor box offsets.\n",
    "      It then filters the resulting boxes through MNS\n",
    "      Inputs:\n",
    "          anchrs - Anchors as Tensor\n",
    "          activs - Activations as Tensor\n",
    "          no_classes - Number of classes (categories)\n",
    "          threshold - Coarse filtering. Default = 0.5\n",
    "      Output:\n",
    "          one_batch_boxes, one_batch_scores as Tuple'''\n",
    "          \n",
    "  p_bboxes, p_classes = activs    # Read p_bboxes: [32, 189,4] Torch.Tensor and  p_classes: [32, 189, 21]  Torch.Tensor from self.learn.pred  \n",
    "  \n",
    "  #scores = torch.sigmoid(p_classes)                    # Calculate the confidence levels, scores, for class predictions [0, 1] \n",
    "  scores = torch.softmax(p_classes, -1)                  # Calculate the confidence levels, scores, for class predictions [0, 1] - Probabilistic \n",
    "  \n",
    "  offset_boxes = activ_decode(p_bboxes, anchrs)    # Return anchors + anchor offsets wiith format (batch, No Items in Batch, 4)\n",
    "\n",
    "  # For each item in batch, and for each class in the item, filter the image by passing it through NMS. Keep preds with IOU > thresshold\n",
    "  one_batch_boxes = []; one_batch_scores = []; one_batch_cls_pred = []  # Agregators at the bath level\n",
    "\n",
    "  for i in range(p_classes.shape[0]):                   # For each image in batch ...\n",
    "    batch_p_boxes = offset_boxes[i]                     # box preds for the current batch\n",
    "    batch_scores = scores[i]                            # Keep scores for the current batch\n",
    "    max_scores, cls_idx = torch.max(batch_scores, 1 )   # Keep batch class indexes\n",
    "    bch_th_mask = max_scores > threshold           # Threshold mask for batch\n",
    "    bch_keep_boxes = batch_p_boxes[bch_th_mask]         #  \"\n",
    "    bch_keep_scores = batch_scores[bch_th_mask]         #  \"\n",
    "    bch_keep_cls_idx = cls_idx[bch_th_mask]\n",
    "\n",
    "    # Agregators per image in a batch\n",
    "    img_boxes = []                                      # Bounding boxes per image\n",
    "    img_scores = []                                     # Scores per image\n",
    "    img_cls_pred = []                                   # Class predictons per image\n",
    "\n",
    "    for c in range (1,no_classes):                      # Loop through each class\n",
    "\n",
    "      cls_mask = bch_keep_cls_idx==c                    # Keep masks for the current class\n",
    "      if cls_mask.sum() == 0: continue                  # Weed out images with no positive class masks\n",
    "\n",
    "      cls_boxes = bch_keep_boxes[cls_mask]              # Keep boxes per image\n",
    "      cls_scores = bch_keep_scores[cls_mask].max(dim=1)[0]    # Keep class scores for the current image\n",
    "  \n",
    "      nms_keep_idx = torchvision.ops.nms(cls_boxes, cls_scores, iou_threshold=0.5)   # Filter images by passing them through NMS\n",
    "\n",
    "      img_boxes += [*cls_boxes[nms_keep_idx]]           # Agregate cls_boxes into tensors for all classes\n",
    "      box_stack = torch.stack(img_boxes,0)              # Transform individual tensors into a single box tensor \n",
    "      img_scores += [*cls_scores[nms_keep_idx]]         # Agregate cls_scores into tensors for all classes \n",
    "      score_stack = torch.stack(img_scores, 0)          # Transform individual tensors into a single score tensor \n",
    "\n",
    "      img_cls_pred += [*tensor([c]*len(nms_keep_idx))]   \n",
    "      cls_pred_stack = torch.stack(img_cls_pred, 0)\n",
    "      \n",
    "      batch_mask = score_stack > threshold              # filter final lists tto be greater than threshold\n",
    "      box_stack = box_stack[batch_mask]                 #   \"\n",
    "      \n",
    "      score_stack = score_stack[batch_mask]             #   \"\n",
    "      cls_pred_stack = cls_pred_stack[batch_mask]       #   \"\n",
    "    if 'box_stack' not in locals(): continue            # Failed to find any valid classes\n",
    "    one_batch_boxes.append(box_stack)                   # Agregate bounding boxes for the batch  \n",
    "    one_batch_scores.append(score_stack)                # Agregate scores for the batch \n",
    "    one_batch_cls_pred.append(cls_pred_stack)\n",
    "\n",
    "  # Pad individual box and score tensors into uniform-sized box and score tensors of shapes: (batch, no 0f items in batch, 4) and  (batch, no 0f items in batch, 21)\n",
    "  one_batch_boxes, one_batch_scores, one_batch_cats = pad_output(one_batch_boxes, one_batch_scores, one_batch_cls_pred, no_classes)\n",
    "\n",
    "  return (one_batch_boxes, one_batch_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "'''\n",
    "def save_file(f, name:Text):\n",
    "  with open(F'/content/drive/My Drive/Colab Notebooks/github/nextai/' + name, 'wb') as fp:\n",
    "    pickle.dump(f, fp)\n",
    "    print ('Done')\n",
    "\n",
    "def load_file(name:Text):\n",
    "  # Read Data Classes for use in predicting external images\n",
    "  with open(F'/content/drive/My Drive/Colab Notebooks/github/nextai/' + name, 'rb') as fp:\n",
    "    return pickle.load(fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "#anchors = load_file('anchors')\n",
    "#activs = load_file(\"activations\")\n",
    "anchors = torch.tensor([1.,2.,3.,4.])   # Fake tensor\n",
    "activs = torch.tensor([5.,6.,7.,8.])     # fake tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_hide\n",
    "one_batch_boxes, one_batch_cats = get_activ_offsets_mns(anchors, activs, no_classes=21, threshold=.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBBox([[[-0.1353, -0.1616,  0.6408,  0.8633]],\n",
       "\n",
       "        [[-0.1353, -0.1616,  0.6408,  0.8633]],\n",
       "\n",
       "        [[-0.1353, -0.1616,  0.6408,  0.8633]]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "one_batch_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMultiCategory([[7],\n",
       "        [7],\n",
       "        [7]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "one_batch_cats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
