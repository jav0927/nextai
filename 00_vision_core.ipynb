{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "  !pip install git+https://github.com/pete88b/nbdev_colab_helper.git\n",
    "  from nbdev_colab_helper.core import *\n",
    "  project_name = 'nextai'\n",
    "  init_notebook(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp vision_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vision_core\n",
    "\n",
    "> Utility methods used in vision training and inference.\n",
    "\n",
    "*   These methods are used to manipulate tensors defining bounding boxes, categories, and anchor boxes.\n",
    "*   Input tensors are are of dimension  (bs, k, 4) for bounding boxes or (bs, k, 21) for categories; where bs = batch size and k = number of rows representing a given image.\n",
    "*   Tensors can run on GPU or CPU, depending on the processing environment \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from nbdev import *\n",
    "#from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r\u001b[K     |█                               | 10kB 26.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 245kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 276kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 286kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 296kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 307kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 317kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 327kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 337kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 348kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 5.2MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install fastai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "from fastai.imports import *\n",
    "from torch import tensor, Tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Automatically sets for GPU or CPU environments\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Convert bounding box coordinates from CTRHW to x1, y1, x2, y2 formats.<br>\n",
    "> IMPORTANT: The method expects the input box tensor to be in CxCyHW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Helper Functions for Predictor Methods\n",
    "def ctrhw2tlbr(boxes:Tensor, set_if_input_is_CxCyWH=False):\n",
    "  ''' Convert bounding box coordinates from CTRHW to x1, y1, x2, y2 formats\n",
    "      IMPORTANT: The method expects the input box tensor to be in CxCyHW.               \n",
    "      Inputs:\n",
    "          Boxes        - torch.tensor of activation bounding boxes\n",
    "          Dim  -         (batch size x Items in batch x 4). It will fail otherwise.\n",
    "          Input Format - Center coord, height, width\n",
    "\n",
    "      Output:\n",
    "          torch.tensor of activation bounding boxes\n",
    "          Dim = (batch size, Items in batch, 4)\n",
    "          Format: x1, y1, x2, y2\n",
    "          '''\n",
    "  if set_if_input_is_CxCyWH: boxes = boxes[:,:,[0,1,3,2]]                    # Adjust the format to CxCyHW (height, width). This is the FASTAI format \n",
    "\n",
    "  x1 = (boxes[:,:,0] - torch.true_divide(boxes[:,:,3],2.)).view(-1,1)\n",
    "  x2 = (boxes[:,:,0] + torch.true_divide(boxes[:,:,3],2.)).view(-1,1)\n",
    "  y1 = (boxes[:,:,1] - torch.true_divide(boxes[:,:,2],2.)).view(-1,1)\n",
    "  y2 = (boxes[:,:,1] + torch.true_divide(boxes[:,:,2],2.)).view(-1,1)\n",
    "  \n",
    "  return torch.cat([x1,y1,x2,y2],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.,  1.],\n",
       "        [-1., -1.,  1.,  1.],\n",
       "        [-1., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#center coords to top-left, bottom-right transformation\n",
    "res = ctrhw2tlbr(torch.tensor([[[0,0,2,2],[0,0,2,2],[0,0,2,2]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def tlbr2cthw(boxes:Tensor, ctrhw=True):\n",
    "  '''Convert top/left bottom/right format `boxes` to center/size corners.\n",
    "      Input: \n",
    "          boxes - torch.Tensor of activations bounding boxes\n",
    "                  Unbounded\n",
    "                  Dim = (batch size, Items in batch, 4)\n",
    "                  Format: top left xy, bottom right xy\n",
    "          ctrhw =  True -  Output is in the format CxCyHW\n",
    "                   False - Output is in the format CxCyWH\n",
    "      Output:\n",
    "                  torch.tensor of activation bounding boxes \n",
    "                  Dim = (batch size, Items in the batch, 4)\n",
    "                  Format: center coord xy, height, width'''\n",
    "  center = torch.true_divide(boxes[:,:, :2] + boxes[:,:, 2:], 2)                     # Calculate box center coord\n",
    "  sizes = torch.abs(boxes[:,:, 2:] - boxes[:,:, :2])                # Calculate box width & height                                         # \n",
    "  results = torch.cat( (center, sizes), 2)\n",
    "  if ctrhw: results = results[:,:,[0,1,3,2]]                        # The correct FASTAI Size format is CxCyHW (height, width)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 2., 2.],\n",
       "         [0., 0., 2., 2.],\n",
       "         [0., 0., 2., 2.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#transform top-left, bottom-right coordinates to center, h, w coordinates\n",
    "res = tlbr2cthw(torch.tensor([[[-1,-1,1,1],[-1,-1,1,1],[-1,-1,1,1]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 2., 2.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#transform top-left, bottom-right coordinates to center, h, w coordinates\n",
    "res = tlbr2cthw(torch.tensor([[[-1,-1,1,1]],[[-1,-1,1,1]],[[-1,-1,1,1]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# \"Return tosender\"\n",
    "res = ctrhw2tlbr(tlbr2cthw(torch.tensor([[[-1,-1,1,1]]]))); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# We apply Decoding With Variance to both activation boxes and anchor boxes to calculate the final bounding boxes. \n",
    "def activ_decode(p_boxes:Tensor, anchors:Tensor):\n",
    "  ''' Decodes box activations into final bounding boxes by calculating predicted anchor offsets, which are then added to anchor boxes\n",
    "        Input:\n",
    "            p_boxes - torcht.tensor of activation bounding boxes\n",
    "                      dim:       (batch, items in batch, 4)\n",
    "                      Format:     top left xy, bottom right xy\n",
    "            anchors - torch.tensor of anchors\n",
    "                      Dim:       (k * no of classes) x 4\n",
    "                      Format:    CxCyWH format\n",
    "        Output:\n",
    "                      torcht.tensor with anchor boxes offset by box activations\n",
    "                      dim:    batch x tems in batch x 4)\n",
    "                      Format: tlbr - top left xy, bottom right xy'''\n",
    "\n",
    "  sigma_xy, sigma_hw = torch.sqrt(torch.tensor([0.1])), torch.sqrt(torch.tensor([0.2]))             # Variances for center and hw coordinates\n",
    "\n",
    "  pb = torch.tanh( p_boxes)                 # Set activations into [-1,1] basis (as used in Fastai) \n",
    " \n",
    "  ctrwh = tlbr2cthw(pb, ctrhw=False)        # Transform box activations from xyxy format to CxCyWH format. \n",
    "\n",
    "  # Calculate offset centers. The sequence is Xp, followed by Yp\n",
    "  offset_centers = ctrwh[:,:,[0,1]].to(device) * sigma_xy.to(device) * anchors[:,[2,3]].to(device) + anchors[:,[0,1]].to(device)\n",
    " \n",
    "  # Calculate offset sizes. The sequence is Wp, followed by Hp\n",
    "  offset_sizes =  torch.exp(ctrwh[:,:,[2,3]].to(device) *sigma_hw.to(device)) *anchors[:,[2,3]].to(device)\n",
    " \n",
    "  # Return format to CxCyHW and then return, switching back to X1Y1X2Y2 format.\n",
    "  return torch.clamp(ctrhw2tlbr(torch.cat([offset_centers, offset_sizes], 2), set_if_input_is_CxCyWH=True).view(*p_boxes.shape), min=-1, max=+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000,  1.0000,  1.0000],\n",
       "         [-0.5825, -0.5825,  0.8233,  0.8233]]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "anchors = torch.tensor([[0.0,0.0,2,2],[0.0,0.0,1,1]]) \n",
    "p_boxes = torch.tensor([[[0.0,0.0,2,2],[0.0,0.0,1,1]]]) \n",
    "res = activ_decode(p_boxes, anchors); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Transform activations into final bounding boxes by calculating the predicted offsets to the anchor boxes\n",
    "def activ_encode(p_boxes:Tensor, anchors:Tensor):\n",
    "  ''' Transforms activations into final bounding boxes by calculating predicted anchor offsets, which are then added to the anchor boxes\n",
    "        Input:\n",
    "            p_boxes - torcht.tensor of activation bounding boxes\n",
    "                      dim:    (batch, items in batch, 4)\n",
    "                      Format: top left xy, bottom right xy\n",
    "        Output:\n",
    "                      torch.tensor \n",
    "  '''\n",
    "  sigma_ctr, sigma_hw = torch.sqrt(torch.tensor([0.1])), torch.sqrt(torch.tensor([0.2]))         # Variances\n",
    "  pb = torch.tanh( p_boxes)                 # Set activations into the basis [-1,1] (as used in Fastai)  pb = torch.tanh(p_boxes[...,:] ) \n",
    "  to_ctrwh = tlbr2cthw(pb, tlbr=False)      # Transform activaions from xyxy format to ctrwh format. This will facilitate offset calculations below\n",
    "  \n",
    "  # Calculate anchors with offsets to serve as predicted bounded boxes\n",
    "  offset_center = sigma_ctr * (to_ctrwh[:,:,[0,1]].to(device) - anchors[:,[0,1]].to(device)) / anchors[:,[2,3]].to(device)\n",
    "  offset_size = torch.log(to_ctrwh[:,:,[2,3]].to(device)/anchors[:,[2,3]].to(device)) / sigma_hw\n",
    "  centers = anchors[:,[0,1]].to(device) + offset_center.to(device)\n",
    "  sizes =   anchors[:,[2,3]].to(device) + offset_size.to(device)\n",
    "\n",
    "  return cthw2corners(torch.cat([centers, sizes], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Strip zero-valued rows from a bounding box tensor\n",
    "def strip_zero_rows(bboxes:Tensor):\n",
    "  ''' Strip zero-valued rows from a bounding box tensor \n",
    "      Input:  bboxes   Bounding boox tensor\n",
    "      Output: b_out    Tensor with data rows\n",
    "              z_out    Tensor with zero-filled rows '''\n",
    "  b_out = []; z_out = []\n",
    "\n",
    "  for rw in torch.arange(bboxes.shape[0]):\n",
    "      cc = bboxes[rw,0:][~(bboxes[rw,0:] == 0.).all(1)]                               # Retain the non all-zero rows of the bounding box  \n",
    "      if cc.nelement() != 0 : b_out.append(cc) \n",
    "      zz = bboxes[rw,0:][(bboxes[rw,0:] == 0.).all(1)]                                # Retain the all-zero rows of the bounding box  \n",
    "      if cc.nelement() == 0 : z_out.append(zz)\n",
    "\n",
    "  #return (b_out, z_out)\n",
    "  return (torch.stack(b_out), torch.stack(z_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]],\n",
      "\n",
      "        [[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]])\n",
      "torch.Size([4, 2, 4])\n",
      "non-zero rows: tensor([[[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]],\n",
      "\n",
      "        [[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]]])\n",
      "zero rows: tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "boxxes = torch.tensor([[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]],[[0,0,0,0],[0,0,0,0]],[[0,0,0,0],[0,0,0,0]]]);boxxes\n",
    "print(boxxes)\n",
    "print(boxxes.shape)\n",
    "strip = strip_zero_rows(boxxes)\n",
    "print(F'non-zero rows: {strip[0]}')\n",
    "print(F'zero rows: {strip[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Graft the all-zero rows back to the bounding box array\n",
    "def graft_zerorows_to_tensor(bboxes:Tensor, zboxes:Tensor):\n",
    "  ''' Graft the all-zero rows back to the bounding box row(s) \n",
    "      Input   bboxes   Bounding boox tensor  \n",
    "              zboxes   Tensor containing the zero-valued rows stripped by strip_zero_rows function\n",
    "      Output  Tensor with data and zero-filled rows of shape[0] = shape bboxes[0} shape.zeroboxes[0]'''\n",
    "\n",
    "  return torch.cat([bboxes, zboxes], dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res = graft_zerorows_to_tensor(torch.tensor([[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]]), torch.tensor([[[0,0,0,0],[0,0,0,0]],[[0,0,0,0],[0,0,0,0]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = graft_zerorows_to_tensor(strip[0], strip[1]);res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Flip a bounding box along the y axis\n",
    "def flip_on_y_axis(bboxes:Tensor):                              \n",
    "    ''' Flip a bounding box along the y axis \n",
    "        Input:   bboxes   Bounding box tensor '''\n",
    "    return bboxes[...,[2,1,0,3]]*torch.tensor([-1.,1.,-1.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0., -0.,  1.])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res = flip_on_y_axis(torch.tensor([0,0,1,1]));res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Flip a bounding box along the x axis\n",
    "def flip_on_x_axis(bboxes:Tensor):                              \n",
    "    ''' Flip a bounding box along the x axis \n",
    "        Input:   bboxes   Bounding boox tensor '''\n",
    "    return bboxes[...,[0,3,2,1]]*torch.tensor([1.,-1.,1.,-1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.,  1., -0.])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res = flip_on_x_axis(torch.tensor([0,0,1,1]));res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def rotate_90_plus(bb:Tensor):\n",
    "  ''' Rotate bounding box(s) by 90 degrees clockwise\n",
    "      Input:   bboxes   Bounding boox tensor '''\n",
    "  return bb[...,[3,0,1,2]]*torch.tensor([-1.,1.,-1.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.,  0., -0.,  1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_plus(torch.tensor([[[0,0,1,1]]])); rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def rotate_90_minus(bb:Tensor):\n",
    "  ''' Rotate bounding box(s) by 90 degrees counterclockwise\n",
    "      Input:   bboxes   Bounding boox tensor in xyxy format'''\n",
    "  return bb[...,[1,2,3,0]]*torch.tensor([1.,-1.,1.,-1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function is insensitive to tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1., 1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_minus(rotate_90_plus(torch.tensor([[[0,0,1,1]]])));rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.,  1., -0.])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_minus(torch.tensor([0,0,1,1])); rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0., -1.,  1., -0.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_minus(torch.tensor([[[0,0,1,1]]])); rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_inference_core.ipynb.\n",
      "Converted 01_anchor_boxes.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
