{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/pete88b/nbdev_colab_helper.git\n",
      "  Cloning https://github.com/pete88b/nbdev_colab_helper.git to /tmp/pip-req-build-ou8zza6m\n",
      "  Running command git clone -q https://github.com/pete88b/nbdev_colab_helper.git /tmp/pip-req-build-ou8zza6m\n",
      "Building wheels for collected packages: nbdev-colab-helper\n",
      "  Building wheel for nbdev-colab-helper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nbdev-colab-helper: filename=nbdev_colab_helper-0.0.1-cp36-none-any.whl size=9648 sha256=4b033bbeb979740e829237ae5c0ce545f1c524de138c676532b9ef886a555ba7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-szn00uw5/wheels/11/52/f4/a49fbdda142e8992bb1695aa9eb30f499294a14cfb4b753fbe\n",
      "Successfully built nbdev-colab-helper\n",
      "Installing collected packages: nbdev-colab-helper\n",
      "Successfully installed nbdev-colab-helper-0.0.1\n",
      "Connecting to google drive\n",
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks/github/nextai\n",
      "pip install git+https://github.com/fastai/nbdev.git\n",
      "   Collecting git+https://github.com/fastai/nbdev.git\n",
      "  Cloning https://github.com/fastai/nbdev.git to /tmp/pip-req-build-nfr5vkun\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (19.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (20.4)\n",
      "Collecting fastcore>=1.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/f6/fe20bfa7d818104f940da81a60e7532090fd87c31f90d4c3b8ae2823d28a/fastcore-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (5.0.7)\n",
      "Requirement already satisfied: nbconvert<6 in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (5.6.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (3.13)\n",
      "Collecting fastscript>=1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/60/e4/7790e3ca100841566fdc1ccee413b9a9d40629d1858d86b1b9ffbc4fa75a/fastscript-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: jupyter_client in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (5.3.5)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from nbdev==1.0.18) (4.10.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->nbdev==1.0.18) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->nbdev==1.0.18) (2.4.7)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev==1.0.18) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev==1.0.18) (4.3.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev==1.0.18) (2.6.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev==1.0.18) (4.6.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (3.1.5)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (0.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (2.11.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (0.8.4)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (2.1.3)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert<6->nbdev==1.0.18) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter_client->nbdev==1.0.18) (2.8.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter_client->nbdev==1.0.18) (5.1.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter_client->nbdev==1.0.18) (19.0.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->nbdev==1.0.18) (5.5.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat>=4.4.0->nbdev==1.0.18) (4.4.2)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert<6->nbdev==1.0.18) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert<6->nbdev==1.0.18) (1.1.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev==1.0.18) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev==1.0.18) (4.8.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev==1.0.18) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev==1.0.18) (49.6.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->nbdev==1.0.18) (1.0.18)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->nbdev==1.0.18) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->nbdev==1.0.18) (0.2.5)\n",
      "Building wheels for collected packages: nbdev\n",
      "  Building wheel for nbdev (setup.py): started\n",
      "  Building wheel for nbdev (setup.py): finished with status 'done'\n",
      "  Created wheel for nbdev: filename=nbdev-1.0.18-cp36-none-any.whl size=55139 sha256=beba7a5a521fe62d67b58a5865fd8cd77c1f1f1be20901038aabf83a7d187572\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w_qqpqam/wheels/92/fb/71/9ef9a3edcda4b045bd45abb28de841f18ab36606a7ff7473d4\n",
      "Successfully built nbdev\n",
      "Installing collected packages: fastcore, fastscript, nbdev\n",
      "Successfully installed fastcore-1.0.9 fastscript-1.0.0 nbdev-1.0.18 \n",
      "/content/drive/My Drive/Colab Notebooks/github/nextai\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "if IN_COLAB:\n",
    "  !pip install git+https://github.com/pete88b/nbdev_colab_helper.git\n",
    "  from nbdev_colab_helper.core import *\n",
    "  project_name = 'nextai'\n",
    "  init_notebook(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference_core\n",
    "\n",
    "> Utility methods used in vision training and inference.\n",
    "\n",
    "*   These methods are used to manipulate tensors defining bounding boxes, categories, and anchor boxes.\n",
    "*   Input tensors are are of dimension  (bs, k, 4) for bounding boxes or (bs, k, 21) for categories; where bs = batch size and k = number of rows representing a given image.\n",
    "*   Tensors can run on GPU or CPU, depending on the processing environment \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r\u001b[K     |█                               | 10kB 5.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 163kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 174kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 184kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 194kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 204kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 215kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 225kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 235kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 245kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 256kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 276kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 286kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 296kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 307kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 317kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 327kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 337kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 348kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 4.2MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!pip install fastai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "from fastai.imports import *\n",
    "from torch import tensor, Tensor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Automatically sets for GPU or CPU environments\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Convert bounding box coordinates from CTRHW to x1, y1, x2, y2 formats.<br>\n",
    "> IMPORTANT: The method expects the input box tensor to be in CxCyHW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Helper Functions for Predictor Methods\n",
    "def ctrhw2tlbr(boxes:Tensor, set_if_input_is_CxCyWH=False):\n",
    "  ''' Convert bounding box coordinates from CTRHW to x1, y1, x2, y2 formats\n",
    "      IMPORTANT: The method expects the input box tensor to be in CxCyHW.               \n",
    "      Inputs:\n",
    "          Boxes        - torch.tensor of activation bounding boxes\n",
    "          Dim  -         (batch size x Items in batch x 4). It will fail otherwise.\n",
    "          Input Format - Center coord, height, width\n",
    "\n",
    "      Output:\n",
    "          torch.tensor of activation bounding boxes\n",
    "          Dim = (batch size, Items in batch, 4)\n",
    "          Format: x1, y1, x2, y2\n",
    "          '''\n",
    "  if set_if_input_is_CxCyWH: boxes = boxes[:,:,[0,1,3,2]]                    # Adjust the format to CxCyHW (height, width). This is the FASTAI format \n",
    "\n",
    "  x1 = (boxes[:,:,0] - torch.true_divide(boxes[:,:,3],2.)).view(-1,1)\n",
    "  x2 = (boxes[:,:,0] + torch.true_divide(boxes[:,:,3],2.)).view(-1,1)\n",
    "  y1 = (boxes[:,:,1] - torch.true_divide(boxes[:,:,2],2.)).view(-1,1)\n",
    "  y2 = (boxes[:,:,1] + torch.true_divide(boxes[:,:,2],2.)).view(-1,1)\n",
    "  \n",
    "  return torch.cat([x1,y1,x2,y2],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.,  1.],\n",
       "        [-1., -1.,  1.,  1.],\n",
       "        [-1., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#center coords to top-left, bottom-right transformation\n",
    "res = ctrhw2tlbr(torch.tensor([[[0,0,2,2],[0,0,2,2],[0,0,2,2]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def tlbr2cthw(boxes:Tensor, ctrhw=True):\n",
    "  '''Convert top/left bottom/right format `boxes` to center/size corners.\n",
    "      Input: \n",
    "          boxes - torch.Tensor of activations bounding boxes\n",
    "                  Unbounded\n",
    "                  Dim = (batch size, Items in batch, 4)\n",
    "                  Format: top left xy, bottom right xy\n",
    "          ctrhw =  True -  Output is in the format CxCyHW\n",
    "                   False - Output is in the format CxCyWH\n",
    "      Output:\n",
    "                  torch.tensor of activation bounding boxes \n",
    "                  Dim = (batch size, Items in the batch, 4)\n",
    "                  Format: center coord xy, height, width'''\n",
    "  center = torch.true_divide(boxes[:,:, :2] + boxes[:,:, 2:], 2)                     # Calculate box center coord\n",
    "  sizes = torch.abs(boxes[:,:, 2:] - boxes[:,:, :2])                # Calculate box width & height                                         # \n",
    "  results = torch.cat( (center, sizes), 2)\n",
    "  if ctrhw: results = results[:,:,[0,1,3,2]]                        # The correct FASTAI Size format is CxCyHW (height, width)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 2., 2.],\n",
       "         [0., 0., 2., 2.],\n",
       "         [0., 0., 2., 2.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#transform top-left, bottom-right coordinates to center, h, w coordinates\n",
    "res = tlbr2cthw(torch.tensor([[[-1,-1,1,1],[-1,-1,1,1],[-1,-1,1,1]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 2., 2.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#transform top-left, bottom-right coordinates to center, h, w coordinates\n",
    "res = tlbr2cthw(torch.tensor([[[-1,-1,1,1]],[[-1,-1,1,1]],[[-1,-1,1,1]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# \"Return tosender\"\n",
    "res = ctrhw2tlbr(tlbr2cthw(torch.tensor([[[-1,-1,1,1]]]))); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# We apply Decoding With Variance to both activation boxes and anchor boxes to calculate the final bounding boxes. \n",
    "def activ_decode(p_boxes:Tensor, anchors:Tensor):\n",
    "  ''' Decodes box activations into final bounding boxes by calculating predicted anchor offsets, which are then added to anchor boxes\n",
    "        Input:\n",
    "            p_boxes - torcht.tensor of activation bounding boxes\n",
    "                      dim:       (batch, items in batch, 4)\n",
    "                      Format:     top left xy, bottom right xy\n",
    "            anchors - torch.tensor of anchors\n",
    "                      Dim:       (k * no of classes) x 4\n",
    "                      Format:    CxCyWH format\n",
    "        Output:\n",
    "                      torcht.tensor with anchor boxes offset by box activations\n",
    "                      dim:    batch x tems in batch x 4)\n",
    "                      Format: tlbr - top left xy, bottom right xy'''\n",
    "\n",
    "  sigma_xy, sigma_hw = torch.sqrt(torch.tensor([0.1])), torch.sqrt(torch.tensor([0.2]))             # Variances for center and hw coordinates\n",
    "\n",
    "  pb = torch.tanh( p_boxes)                 # Set activations into [-1,1] basis (as used in Fastai) \n",
    "  print(pb.is_cuda)\n",
    "  ctrwh = tlbr2cthw(pb, ctrhw=False)        # Transform box activations from xyxy format to CxCyWH format. \n",
    "\n",
    "  # Calculate offset centers. The sequence is Xp, followed by Yp\n",
    "  offset_centers = ctrwh[:,:,[0,1]].to(device) * sigma_xy.to(device) * anchors[:,[2,3]].to(device) + anchors[:,[0,1]].to(device)\n",
    " \n",
    "  # Calculate offset sizes. The sequence is Wp, followed by Hp\n",
    "  offset_sizes =  torch.exp(ctrwh[:,:,[2,3]].to(device) *sigma_hw.to(device)) *anchors[:,[2,3]].to(device)\n",
    " \n",
    "  # Return format to CxCyHW and then return, switching back to X1Y1X2Y2 format.\n",
    "  return torch.clamp(ctrhw2tlbr(torch.cat([offset_centers, offset_sizes], 2), set_if_input_is_CxCyWH=True).view(*p_boxes.shape), min=-1, max=+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000,  1.0000,  1.0000],\n",
       "         [-0.5825, -0.5825,  0.8233,  0.8233]]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "anchors = torch.tensor([[0.0,0.0,2,2],[0.0,0.0,1,1]]) \n",
    "p_boxes = torch.tensor([[[0.0,0.0,2,2],[0.0,0.0,1,1]]]) \n",
    "res = activ_decode(p_boxes, anchors); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Transform activations into final bounding boxes by calculating the predicted offsets to the anchor boxes\n",
    "def activ_encode(p_boxes:Tensor, anchors:Tensor):\n",
    "  ''' Transforms activations into final bounding boxes by calculating predicted anchor offsets, which are then added to the anchor boxes\n",
    "        Input:\n",
    "            p_boxes - torcht.tensor of activation bounding boxes\n",
    "                      dim:    (batch, items in batch, 4)\n",
    "                      Format: top left xy, bottom right xy\n",
    "        Output:\n",
    "                      torch.tensor \n",
    "  '''\n",
    "  sigma_ctr, sigma_hw = torch.sqrt(torch.tensor([0.1])), torch.sqrt(torch.tensor([0.2]))         # Variances\n",
    "  pb = torch.tanh( p_boxes)                 # Set activations into the basis [-1,1] (as used in Fastai)  pb = torch.tanh(p_boxes[...,:] ) \n",
    "  to_ctrwh = tlbr2cthw(pb, tlbr=False)      # Transform activaions from xyxy format to ctrwh format. This will facilitate offset calculations below\n",
    "  \n",
    "  # Calculate anchors with offsets to serve as predicted bounded boxes\n",
    "  offset_center = sigma_ctr * (to_ctrwh[:,:,[0,1]].to(device) - anchors[:,[0,1]].to(device)) / anchors[:,[2,3]].to(device)\n",
    "  offset_size = torch.log(to_ctrwh[:,:,[2,3]].to(device)/anchors[:,[2,3]].to(device)) / sigma_hw\n",
    "  centers = anchors[:,[0,1]].to(device) + offset_center.to(device)\n",
    "  sizes =   anchors[:,[2,3]].to(device) + offset_size.to(device)\n",
    "\n",
    "  return cthw2corners(torch.cat([centers, sizes], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Strip zero-valued rows from a bounding box tensor\n",
    "def strip_zero_rows(bboxes:Tensor):\n",
    "  ''' Strip zero-valued rows from a bounding box tensor \n",
    "      Input:  bboxes   Bounding boox tensor\n",
    "      Output: b_out    Tensor with data rows\n",
    "              z_out    Tensor with zero-filled rows '''\n",
    "  b_out = []; z_out = []\n",
    "\n",
    "  for rw in torch.arange(bboxes.shape[0]):\n",
    "      cc = bboxes[rw,0:][~(bboxes[rw,0:] == 0.).all(1)]                               # Retain the non all-zero rows of the bounding box  \n",
    "      if cc.nelement() != 0 : b_out.append(cc) \n",
    "      zz = bboxes[rw,0:][(bboxes[rw,0:] == 0.).all(1)]                                # Retain the all-zero rows of the bounding box  \n",
    "      if cc.nelement() == 0 : z_out.append(zz)\n",
    "\n",
    "  #return (b_out, z_out)\n",
    "  return (torch.stack(b_out), torch.stack(z_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]],\n",
      "\n",
      "        [[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]])\n",
      "torch.Size([4, 2, 4])\n",
      "non-zero rows: tensor([[[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]],\n",
      "\n",
      "        [[1, 2, 3, 4],\n",
      "         [1, 2, 3, 4]]])\n",
      "zero rows: tensor([[[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "boxxes = torch.tensor([[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]],[[0,0,0,0],[0,0,0,0]],[[0,0,0,0],[0,0,0,0]]]);boxxes\n",
    "print(boxxes)\n",
    "print(boxxes.shape)\n",
    "strip = strip_zero_rows(boxxes)\n",
    "print(F'non-zero rows: {strip[0]}')\n",
    "print(F'zero rows: {strip[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Graft the all-zero rows back to the bounding box array\n",
    "def graft_zerorows_to_tensor(bboxes:Tensor, zboxes:Tensor):\n",
    "  ''' Graft the all-zero rows back to the bounding box row(s) \n",
    "      Input   bboxes   Bounding boox tensor  \n",
    "              zboxes   Tensor containing the zero-valued rows stripped by strip_zero_rows function\n",
    "      Output  Tensor with data and zero-filled rows of shape[0] = shape bboxes[0} shape.zeroboxes[0]'''\n",
    "\n",
    "  return torch.cat([bboxes, zboxes], dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res = graft_zerorows_to_tensor(torch.tensor([[[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4]]]), torch.tensor([[[0,0,0,0],[0,0,0,0]],[[0,0,0,0],[0,0,0,0]]])); res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 2, 3, 4],\n",
       "         [1, 2, 3, 4],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = graft_zerorows_to_tensor(strip[0], strip[1]);res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Flip a bounding box along the y axis\n",
    "def flip_on_y_axis(bboxes:Tensor):                              \n",
    "    ''' Flip a bounding box along the y axis \n",
    "        Input:   bboxes   Bounding box tensor '''\n",
    "    return bboxes[...,[2,1,0,3]]*torch.tensor([-1.,1.,-1.,1.]).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0., -0.,  1.])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res = flip_on_y_axis(torch.tensor([0,0,1,1]));res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# Flip a bounding box along the x axis\n",
    "def flip_on_x_axis(bboxes:Tensor):                              \n",
    "    ''' Flip a bounding box along the x axis \n",
    "        Input:   bboxes   Bounding boox tensor '''\n",
    "    return bboxes[...,[0,3,2,1]]*torch.tensor([1.,-1.,1.,-1.]).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.,  1., -0.])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "res = flip_on_x_axis(torch.tensor([0,0,1,1]));res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def rotate_90_plus(bb:Tensor):\n",
    "  ''' Rotate bounding box(s) by 90 degrees clockwise\n",
    "      Input:   bboxes   Bounding boox tensor '''\n",
    "  return bb[...,[3,0,1,2]]*torch.tensor([-1.,1.,-1.,1.]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.,  0., -0.,  1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_plus(torch.tensor([[[0,0,1,1]]])); rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def rotate_90_minus(bb:Tensor):\n",
    "  ''' Rotate bounding box(s) by 90 degrees counterclockwise\n",
    "      Input:   bboxes   Bounding boox tensor in xyxy format'''\n",
    "  return bb[...,[1,2,3,0]]*torch.tensor([1.,-1.,1.,-1.]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function is insensitive to tensor dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1., 1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_minus(rotate_90_plus(torch.tensor([[[0,0,1,1]]])));rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.,  1., -0.])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_minus(torch.tensor([0,0,1,1])); rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0., -1.,  1., -0.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#Insensitive to tensor dimensions\n",
    "rot = rotate_90_minus(torch.tensor([[[0,0,1,1]]])); rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_inference_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       ""
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting: /content/drive/My Drive/Colab Notebooks/github/nextai/index.ipynb\n",
      "converting: /content/drive/My Drive/Colab Notebooks/github/nextai/00_inference_core.ipynb\n",
      "converting /content/drive/My Drive/Colab Notebooks/github/nextai/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "from nbdev.cli import nbdev_build_docs\n",
    "nbdev_build_docs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
